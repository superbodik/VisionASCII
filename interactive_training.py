import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import numpy as np
import cv2
from collections import deque
import time
import os

class DynamicObjectClassifier(nn.Module):
    def __init__(self, num_classes=4):  # 0=noise/shadow, 1=person, 2=vehicle, 3=unknown
        super().__init__()
        
        # –ö–æ–Ω–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.features = nn.Sequential(
            # –ü–µ—Ä–≤—ã–π –±–ª–æ–∫
            nn.Conv2d(3, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),  # —É–±—Ä–∞–ª inplace=True
            nn.Conv2d(32, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),  # —É–±—Ä–∞–ª inplace=True
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.1),
            
            # –í—Ç–æ—Ä–æ–π –±–ª–æ–∫
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),  # —É–±—Ä–∞–ª inplace=True
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),  # —É–±—Ä–∞–ª inplace=True
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.15),
            
            # –¢—Ä–µ—Ç–∏–π –±–ª–æ–∫
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),  # —É–±—Ä–∞–ª inplace=True
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),  # —É–±—Ä–∞–ª inplace=True
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(0.2),
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø—É–ª–∏–Ω–≥ –¥–ª—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            nn.AdaptiveAvgPool2d((4, 4))
        )
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–ª—å–∫–æ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.visual_classifier = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(128 * 4 * 4, 256),
            nn.ReLU(),  # —É–±—Ä–∞–ª inplace=True
            nn.Dropout(0.4),
            nn.Linear(256, 64),
            nn.ReLU()   # —É–±—Ä–∞–ª inplace=True
        )
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ)
        self.geometry_features = nn.Sequential(
            nn.Linear(8, 32),  # x, y, w, h, aspect_ratio, area, density, position_ratio
            nn.ReLU(),  # —É–±—Ä–∞–ª inplace=True
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU()   # —É–±—Ä–∞–ª inplace=True
        )
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        self.final_classifier = nn.Sequential(
            nn.Linear(64 + 16, 32),  # 64 –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö + 16 –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö = 80
            nn.ReLU(),  # —É–±—Ä–∞–ª inplace=True
            nn.Dropout(0.3),
            nn.Linear(32, num_classes)
        )
        
    def extract_geometry_features(self, bbox, frame_shape):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ bounding box"""
        x, y, w, h = bbox
        frame_h, frame_w = frame_shape[:2]
        
        # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
        frame_w = max(frame_w, 1)
        frame_h = max(frame_h, 1)
        h = max(h, 1)
        
        features = [
            x / frame_w,  # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è x –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞
            y / frame_h,  # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è y –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞
            w / frame_w,  # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —à–∏—Ä–∏–Ω–∞
            h / frame_h,  # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –≤—ã—Å–æ—Ç–∞
            w / h,  # —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å—Ç–æ—Ä–æ–Ω
            (w * h) / (frame_w * frame_h),  # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø–ª–æ—â–∞–¥—å
            (w * h) / (w + h),  # –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
            (y + h/2) / frame_h  # –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –ø–æ –≤–µ—Ä—Ç–∏–∫–∞–ª–∏
        ]
        
        return torch.tensor(features, dtype=torch.float32)
    
    def forward(self, image, bbox=None, frame_shape=None):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        visual_features = self.features(image)
        visual_features = visual_features.view(visual_features.size(0), -1)
        visual_output = self.visual_classifier(visual_features)
        
        # –ï—Å–ª–∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if bbox is None or frame_shape is None:
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–µ–≤—ã–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            batch_size = image.size(0)
            geo_features = torch.zeros(batch_size, 16, device=image.device)
        else:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            batch_size = image.size(0)
            
            if batch_size == 1:
                # –î–ª—è –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ bbox —è–≤–ª—è–µ—Ç—Å—è –∫–æ—Ä—Ç–µ–∂–µ–º (x, y, w, h)
                try:
                    geo_features = self.extract_geometry_features(bbox, frame_shape).unsqueeze(0)
                except:
                    geo_features = torch.zeros(1, 8, device=image.device)
            else:
                # –î–ª—è –±–∞—Ç—á–∞ bbox –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º –∫–æ—Ä—Ç–µ–∂–µ–π
                geo_features_list = []
                for i in range(batch_size):
                    try:
                        if isinstance(bbox, list) and len(bbox) > i:
                            geo_feat = self.extract_geometry_features(bbox[i], frame_shape)
                        else:
                            geo_feat = torch.zeros(8, device=image.device)
                        geo_features_list.append(geo_feat)
                    except:
                        geo_features_list.append(torch.zeros(8, device=image.device))
                        
                geo_features = torch.stack(geo_features_list)
            
            geo_features = geo_features.to(image.device)
            geo_features = self.geometry_features(geo_features)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        combined = torch.cat([visual_output, geo_features], dim=1)
        final_output = self.final_classifier(combined)
        
        return final_output

class ShadowDetector:
    """–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä —Ç–µ–Ω–µ–π"""
    
    @staticmethod
    def is_likely_shadow(image_patch, bbox):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ–±–ª–∞—Å—Ç—å —Ç–µ–Ω—å—é (–±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ)"""
        if image_patch is None or image_patch.size == 0:
            return False
            
        try:
            # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–∞ - –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã —Å–∫–æ—Ä–µ–µ —Ç–µ–Ω–∏
            x, y, w, h = bbox
            area = w * h
            aspect_ratio = w / h if h > 0 else 1
            
            # –ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π –∏–ª–∏ —Å—Ç—Ä–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º—ã
            if area < 400 or aspect_ratio > 8 or aspect_ratio < 0.1:
                return True
            
            # –ê–Ω–∞–ª–∏–∑ —Ü–≤–µ—Ç–∞ –∏ —è—Ä–∫–æ—Å—Ç–∏
            gray = cv2.cvtColor(image_patch, cv2.COLOR_BGR2GRAY)
            mean_brightness = np.mean(gray)
            std_brightness = np.std(gray)
            
            # –û—á–µ–Ω—å —Ç–µ–º–Ω—ã–µ –∏ –æ–¥–Ω–æ—Ä–æ–¥–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏
            if mean_brightness < 40 and std_brightness < 15:
                return True
                
            # –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç–∏
            hsv = cv2.cvtColor(image_patch, cv2.COLOR_BGR2HSV)
            saturation = hsv[:, :, 1]
            mean_saturation = np.mean(saturation)
            
            # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –í–°–ï –∫—Ä–∏—Ç–µ—Ä–∏–∏ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ç–µ–Ω—å
            is_very_dark = mean_brightness < 50
            very_low_contrast = std_brightness < 12
            very_low_saturation = mean_saturation < 15
            
            # –¢–µ–Ω—å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –í–°–ï 3 –∫—Ä–∏—Ç–µ—Ä–∏—è –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è
            return is_very_dark and very_low_contrast and very_low_saturation
            
        except Exception as e:
            return False  # –ü—Ä–∏ –æ—à–∏–±–∫–µ –Ω–µ —Å—á–∏—Ç–∞–µ–º —Ç–µ–Ω—å—é
    
    @staticmethod
    def analyze_shadow_context(frame, bbox):
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ - –±–æ–ª–µ–µ –º—è–≥–∫–∏–π"""
        try:
            x, y, w, h = bbox
            area = w * h
            
            # –ù–µ –ø—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫ –æ–±—ä–µ–∫—Ç–∞–º —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            if area > 800:  # –õ—é–¥–∏ –æ–±—ã—á–Ω–æ –±–æ–ª—å—à–µ 800 –ø–∏–∫—Å–µ–ª–µ–π
                return False
                
            frame_h, frame_w = frame.shape[:2]
            
            # –†–∞—Å—à–∏—Ä—è–µ–º –æ–±–ª–∞—Å—Ç—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context_margin = 15
            x1 = max(0, x - context_margin)
            y1 = max(0, y - context_margin)
            x2 = min(frame_w, x + w + context_margin)
            y2 = min(frame_h, y + h + context_margin)
            
            if x2 <= x1 or y2 <= y1:
                return False
                
            context_region = frame[y1:y2, x1:x2]
            object_region = frame[y:y+h, x:x+w]
            
            context_brightness = np.mean(cv2.cvtColor(context_region, cv2.COLOR_BGR2GRAY))
            object_brightness = np.mean(cv2.cvtColor(object_region, cv2.COLOR_BGR2GRAY))
            
            # –¢–æ–ª—å–∫–æ –æ—á–µ–Ω—å —Ç–µ–º–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            brightness_ratio = object_brightness / (context_brightness + 1)
            return brightness_ratio < 0.4  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –∫—Ä–∏—Ç–µ—Ä–∏–π
            
        except:
            return False

class AdvancedObjectClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
    
    @staticmethod
    def classify_by_rules(bbox, frame_shape, movement_speed=0):
        """–ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å —á–µ—Ç–∫–∏–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏"""
        x, y, w, h = bbox
        frame_h, frame_w = frame_shape[:2]
        
        area = w * h
        aspect_ratio = w / h if h > 0 else 1
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–∞–¥—Ä–∞
        relative_width = w / frame_w
        relative_height = h / frame_h
        center_y = (y + h/2) / frame_h
        
        print(f"üîç –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–∫—Ç–∞: {w}x{h}px, area={area}, ratio={aspect_ratio:.2f}, speed={movement_speed:.1f}")
        
        # === –ß–ï–¢–ö–ò–ï –ö–†–ò–¢–ï–†–ò–ò –î–õ–Ø –ú–ê–®–ò–ù ===
        vehicle_score = 0
        vehicle_reasons = []
        
        # 1. –†–∞–∑–º–µ—Ä –º–∞—à–∏–Ω—ã (—Å—Ç—Ä–æ–≥–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏)
        if area >= 4000:
            vehicle_score += 4
            vehicle_reasons.append(f"Large area ({area})")
        elif area >= 2500:
            vehicle_score += 2
            vehicle_reasons.append(f"Medium area ({area})")
            
        # 2. –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –º–∞—à–∏–Ω—ã
        if 1.5 <= aspect_ratio <= 3.5:
            vehicle_score += 4
            vehicle_reasons.append(f"Car proportions ({aspect_ratio:.2f})")
        elif 1.2 <= aspect_ratio <= 4.0:
            vehicle_score += 2
            vehicle_reasons.append(f"Wide proportions ({aspect_ratio:.2f})")
            
        # 3. –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –º–∞—à–∏–Ω—ã
        if w >= 90 and h >= 60:
            vehicle_score += 3
            vehicle_reasons.append(f"Car size ({w}x{h})")
        elif w >= 70 and h >= 45:
            vehicle_score += 1
            vehicle_reasons.append(f"Medium size ({w}x{h})")
            
        # 4. –°–∫–æ—Ä–æ—Å—Ç—å –º–∞—à–∏–Ω—ã
        if movement_speed > 4.0:
            vehicle_score += 3
            vehicle_reasons.append(f"Fast speed ({movement_speed:.1f})")
        elif movement_speed > 2.5:
            vehicle_score += 2
            vehicle_reasons.append(f"Car speed ({movement_speed:.1f})")
            
        # 5. –ü–æ–∑–∏—Ü–∏—è –Ω–∞ –¥–æ—Ä–æ–≥–µ
        if center_y >= 0.4:
            vehicle_score += 1
            vehicle_reasons.append("Road position")
            
        # === –ß–ï–¢–ö–ò–ï –ö–†–ò–¢–ï–†–ò–ò –î–õ–Ø –õ–Æ–î–ï–ô ===
        person_score = 0
        person_reasons = []
        
        # 1. –†–∞–∑–º–µ—Ä —á–µ–ª–æ–≤–µ–∫–∞
        if 800 <= area <= 6000:
            person_score += 4
            person_reasons.append(f"Person area ({area})")
        elif 500 <= area <= 10000:
            person_score += 2
            person_reasons.append(f"Human-like area ({area})")
            
        # 2. –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞  
        if 0.4 <= aspect_ratio <= 0.8:
            person_score += 4
            person_reasons.append(f"Person proportions ({aspect_ratio:.2f})")
        elif 0.3 <= aspect_ratio <= 1.0:
            person_score += 2
            person_reasons.append(f"Vertical proportions ({aspect_ratio:.2f})")
            
        # 3. –†–∞–∑–º–µ—Ä—ã —á–µ–ª–æ–≤–µ–∫–∞
        if 25 <= w <= 80 and 50 <= h <= 200:
            person_score += 3
            person_reasons.append(f"Person size ({w}x{h})")
        elif 20 <= w <= 100 and 40 <= h <= 250:
            person_score += 1
            person_reasons.append(f"Human-like size ({w}x{h})")
            
        # 4. –°–∫–æ—Ä–æ—Å—Ç—å —á–µ–ª–æ–≤–µ–∫–∞
        if 0.5 <= movement_speed <= 2.5:
            person_score += 3
            person_reasons.append(f"Walking speed ({movement_speed:.1f})")
        elif 0.2 <= movement_speed <= 4.0:
            person_score += 1
            person_reasons.append(f"Human speed ({movement_speed:.1f})")
        elif movement_speed > 5.0:
            person_score -= 2
            person_reasons.append(f"Too fast for person ({movement_speed:.1f})")
            
        # === –ö–†–ò–¢–ï–†–ò–ò –î–õ–Ø –¢–ï–ù–ï–ô ===
        shadow_score = 0
        shadow_reasons = []
        
        # 1. –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã
        if area < 400:
            shadow_score += 3
            shadow_reasons.append(f"Very small ({area})")
            
        # 2. –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏
        if aspect_ratio > 6 or aspect_ratio < 0.15:
            shadow_score += 3
            shadow_reasons.append(f"Extreme ratio ({aspect_ratio:.2f})")
            
        # 3. –û—á–µ–Ω—å —É–∑–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã
        if w < 15 or h < 15:
            shadow_score += 2
            shadow_reasons.append(f"Too narrow ({w}x{h})")
            
        # === –ü–†–ò–ù–Ø–¢–ò–ï –†–ï–®–ï–ù–ò–Ø –° –õ–û–ì–ò–†–û–í–ê–ù–ò–ï–ú ===
        
        print(f"üöó Vehicle score: {vehicle_score} - {vehicle_reasons}")
        print(f"üë§ Person score: {person_score} - {person_reasons}")
        print(f"üåë Shadow score: {shadow_score} - {shadow_reasons}")
        
        # –ß–µ—Ç–∫–∏–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
        if shadow_score >= 5:
            print(f"‚úÖ DECISION: SHADOW (score: {shadow_score})")
            return 0, 0.8 + min(0.15, shadow_score * 0.02)
            
        elif vehicle_score >= 8:
            confidence = 0.85 + min(0.1, vehicle_score * 0.01)
            print(f"‚úÖ DECISION: VEHICLE (score: {vehicle_score}, confidence: {confidence:.2f})")
            return 2, confidence
            
        elif person_score >= 7:
            confidence = 0.8 + min(0.15, person_score * 0.02)
            print(f"‚úÖ DECISION: PERSON (score: {person_score}, confidence: {confidence:.2f})")
            return 1, confidence
            
        elif vehicle_score >= 5 and vehicle_score > person_score + 2:
            confidence = 0.6 + vehicle_score * 0.03
            print(f"‚úÖ DECISION: VEHICLE (lower confidence, score: {vehicle_score})")
            return 2, confidence
            
        elif person_score >= 4 and person_score > vehicle_score + 1:
            confidence = 0.6 + person_score * 0.03
            print(f"‚úÖ DECISION: PERSON (lower confidence, score: {person_score})")
            return 1, confidence
            
        elif shadow_score >= 2:
            print(f"‚úÖ DECISION: SHADOW (low confidence, score: {shadow_score})")
            return 0, 0.6
            
        else:
            print(f"‚ùì DECISION: UNKNOWN (vehicle: {vehicle_score}, person: {person_score})")
            return 3, 0.4
    """–î–µ—Ç–µ–∫—Ç–æ—Ä —Ç–µ–Ω–µ–π –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
    
    @staticmethod
    def is_likely_shadow(image_patch, bbox):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ–±–ª–∞—Å—Ç—å —Ç–µ–Ω—å—é"""
        if image_patch is None or image_patch.size == 0:
            return False
            
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ü–≤–µ—Ç–æ–≤—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
        hsv = cv2.cvtColor(image_patch, cv2.COLOR_BGR2HSV)
        lab = cv2.cvtColor(image_patch, cv2.COLOR_BGR2LAB)
        
        # –ê–Ω–∞–ª–∏–∑ —è—Ä–∫–æ—Å—Ç–∏
        gray = cv2.cvtColor(image_patch, cv2.COLOR_BGR2GRAY)
        mean_brightness = np.mean(gray)
        std_brightness = np.std(gray)
        
        # –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç–∏
        saturation = hsv[:, :, 1]
        mean_saturation = np.mean(saturation)
        
        # –ê–Ω–∞–ª–∏–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
        mean_gradient = np.mean(gradient_magnitude)
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ —Ç–µ–Ω–∏
        is_dark = mean_brightness < 80  # —Ç–µ–º–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
        low_contrast = std_brightness < 20  # –Ω–∏–∑–∫–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç
        low_saturation = mean_saturation < 30  # –Ω–∏–∑–∫–∞—è –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å
        weak_edges = mean_gradient < 15  # —Å–ª–∞–±—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã
        
        # –¢–µ–Ω—å, –µ—Å–ª–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
        shadow_score = sum([is_dark, low_contrast, low_saturation, weak_edges])
        return shadow_score >= 3

class DynamicLearner:
    """–°–∏—Å—Ç–µ–º–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–º–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏"""
    
    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        self.criterion = nn.CrossEntropyLoss()
        
        # –ë—É—Ñ–µ—Ä—ã –¥–ª—è –æ–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è (—É–≤–µ–ª–∏—á–µ–Ω—ã)
        self.image_buffer = deque(maxlen=200)  # –ë–æ–ª—å—à–µ –æ–±—Ä–∞–∑—Ü–æ–≤
        self.label_buffer = deque(maxlen=200)
        self.bbox_buffer = deque(maxlen=200)
        self.frame_shape_buffer = deque(maxlen=200)
        self.confidence_buffer = deque(maxlen=200)  # –ù–æ–≤—ã–π –±—É—Ñ–µ—Ä –¥–ª—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        self.learning_stats = {
            'updates': 0,
            'accuracy': 0.0,
            'last_update': time.time(),
            'total_samples': 0,
            'manual_samples': 0,
            'correction_samples': 0
        }
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        self.auto_update_threshold = 5  # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ 5 –Ω–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
        self.confidence_threshold = 0.8
        self.samples_since_update = 0
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ–±—É—á–µ–Ω–∏—è
        self.sample_weights = {
            'manual': 2.0,      # –†—É—á–Ω—ã–µ –≤—ã–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–µ–µ
            'correction': 1.5,  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∞–∂–Ω–µ–µ
            'auto': 1.0        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        }
        
    def add_sample(self, image_patch, bbox, frame_shape, predicted_class, confidence, true_label=None, sample_type='auto'):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –æ–±—Ä–∞–∑–µ—Ü –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —Ç–∏–ø–æ–º –æ–±—Ä–∞–∑—Ü–∞ –∏ –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫"""
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–∏–Ω–Ω—É—é –º–µ—Ç–∫—É
            final_label = true_label if true_label is not None else predicted_class
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∞ –≤—ã—Å–æ–∫–æ —É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            if true_label is None and confidence > self.confidence_threshold and sample_type == 'auto':
                final_label = predicted_class
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–µ–Ω–∏
            if true_label is None and sample_type == 'auto':
                try:
                    if ShadowDetector.is_likely_shadow(image_patch, bbox):
                        final_label = 0  # –∫–ª–∞—Å—Å "—à—É–º/—Ç–µ–Ω—å"
                except:
                    pass  # –ï—Å–ª–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä —Ç–µ–Ω–µ–π –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
            
            if final_label is not None:
                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫
                transform = transforms.Compose([
                    transforms.ToPILImage(),
                    transforms.Resize((64, 64)),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                       std=[0.229, 0.224, 0.225])
                ])
                
                if len(image_patch.shape) == 3 and image_patch.size > 0:
                    try:
                        # –ö–ª–æ–Ω–∏—Ä—É–µ–º image_patch –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è inplace –æ–ø–µ—Ä–∞—Ü–∏–π
                        image_copy = image_patch.copy()
                        image_tensor = transform(image_copy)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–µ–Ω–∑–æ—Ä –≤–∞–ª–∏–¥–Ω—ã–π
                        if not torch.isnan(image_tensor).any() and not torch.isinf(image_tensor).any():
                            # –ö–ª–æ–Ω–∏—Ä—É–µ–º —Ç–µ–Ω–∑–æ—Ä –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                            self.image_buffer.append(image_tensor.clone())
                            self.label_buffer.append(final_label)
                            self.bbox_buffer.append(bbox)
                            self.frame_shape_buffer.append(frame_shape)
                            self.confidence_buffer.append((confidence, sample_type))
                            
                            self.learning_stats['total_samples'] += 1
                            if sample_type == 'manual':
                                self.learning_stats['manual_samples'] += 1
                            elif sample_type == 'correction':
                                self.learning_stats['correction_samples'] += 1
                            
                            self.samples_since_update += 1
                            
                            # –û–±—É—á–µ–Ω–∏–µ –µ—Å–ª–∏ –Ω–∞–∫–æ–ø–∏–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ —ç—Ç–æ –≤–∞–∂–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü
                            should_update = (
                                self.samples_since_update >= self.auto_update_threshold or
                                sample_type in ['manual', 'correction'] or
                                (len(self.image_buffer) >= 10 and time.time() - self.learning_stats['last_update'] > 3.0)
                            )
                            
                            if should_update:
                                self.update_model()
                        else:
                            print("‚ö†Ô∏è Invalid tensor detected, skipping sample")
                            
                    except Exception as transform_error:
                        print(f"‚ö†Ô∏è Error transforming image: {transform_error}")
                        
        except Exception as e:
            print(f"‚ö†Ô∏è Error adding sample: {e}")
            # –û—á–∏—â–∞–µ–º CUDA –∫—ç—à –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    def update_model(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã—Ö –æ—à–∏–±–æ–∫"""
        if len(self.image_buffer) < 5:
            return
        
        try:
            self.model.train()
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞—Ç—á–∞ —Å —É—á–µ—Ç–æ–º –≤–∞–∂–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–∑—Ü–æ–≤
            available_samples = len(self.image_buffer)
            batch_size = min(16, available_samples)
            
            # –°–æ–∑–¥–∞–µ–º –≤–µ—Å–∞ –¥–ª—è –≤—ã–±–æ—Ä–∫–∏ (–≤–∞–∂–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã —á–∞—â–µ –ø–æ–ø–∞–¥–∞—é—Ç –≤ –±–∞—Ç—á)
            sample_weights_list = []
            for i in range(available_samples):
                confidence, sample_type = self.confidence_buffer[i]
                weight = self.sample_weights.get(sample_type, 1.0)
                
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å –¥–ª—è –≤—ã—Å–æ–∫–æ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
                if confidence > 0.9:
                    weight *= 1.2
                elif confidence > 0.8:
                    weight *= 1.1
                    
                sample_weights_list.append(weight)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
            total_weight = sum(sample_weights_list)
            if total_weight > 0:
                sample_probs = [w / total_weight for w in sample_weights_list]
            else:
                sample_probs = [1.0 / available_samples] * available_samples
            
            # –í—ã–±–∏—Ä–∞–µ–º –æ–±—Ä–∞–∑—Ü—ã —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤
            try:
                indices = np.random.choice(available_samples, batch_size, 
                                         replace=False, p=sample_probs)
            except:
                indices = np.random.choice(available_samples, batch_size, replace=False)
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è inplace –æ–ø–µ—Ä–∞—Ü–∏–π
            images = torch.stack([self.image_buffer[i].clone() for i in indices]).to(self.device)
            labels = torch.tensor([self.label_buffer[i] for i in indices], dtype=torch.long).to(self.device)
            
            # –î–ª—è –±–∞—Ç—á–∞ —Å–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ bbox'–æ–≤ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–Ω—É frame_shape
            bboxes = [self.bbox_buffer[i] for i in indices]
            frame_shapes = self.frame_shape_buffer[indices[0]]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é frame_shape
            
            # –û—á–∏—â–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
            self.optimizer.zero_grad()
            
            # –û–±—É—á–µ–Ω–∏–µ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —ç–ø–æ—Ö–∞–º–∏ –¥–ª—è –≤–∞–∂–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
            num_epochs = 1
            manual_or_correction_in_batch = any(
                self.confidence_buffer[i][1] in ['manual', 'correction'] for i in indices
            )
            if manual_or_correction_in_batch:
                num_epochs = 2  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –≤–∞–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            
            total_loss = 0
            for epoch in range(num_epochs):
                # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –æ—á–∏—â–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É
                self.optimizer.zero_grad()
                
                # –§–æ—Ä–≤–∞—Ä–¥ –ø–∞—Å—Å
                outputs = self.model(images, bboxes, frame_shapes)
                loss = self.criterion(outputs, labels)
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –∫ loss –¥–ª—è –≤–∞–∂–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
                if manual_or_correction_in_batch:
                    loss = loss * 1.2  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º loss –¥–ª—è –≤–∞–∂–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
                
                # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                loss.backward()
                
                # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –∫–ª–∏–ø–ø–∏–Ω–≥ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
                self.optimizer.step()
                total_loss += loss.item()
                
                # –û—á–∏—â–∞–µ–º –∫—ç—à CUDA –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            with torch.no_grad():
                _, predicted = torch.max(outputs.data, 1)
                accuracy = (predicted == labels).float().mean().item()
                
            self.learning_stats['updates'] += 1
            self.learning_stats['accuracy'] = accuracy
            self.learning_stats['last_update'] = time.time()
            self.samples_since_update = 0
            
            avg_loss = total_loss / num_epochs
            print(f"üß† Model updated: Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}, "
                  f"Updates={self.learning_stats['updates']}, "
                  f"Samples={self.learning_stats['total_samples']} "
                  f"(M:{self.learning_stats['manual_samples']}, "
                  f"C:{self.learning_stats['correction_samples']})")
            
            self.model.eval()
            
        except RuntimeError as e:
            if "inplace operation" in str(e):
                print("‚ö†Ô∏è Gradient error detected, skipping this update...")
                # –û—á–∏—â–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
                self.optimizer.zero_grad()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                self.model.eval()
            else:
                print(f"‚ùå Training error: {e}")
                self.model.eval()
        except Exception as e:
            print(f"‚ùå Unexpected training error: {e}")
            self.model.eval()
    
    def force_update(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if len(self.image_buffer) > 0:
            print("üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
            self.update_model()
        else:
            print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
    
    def get_learning_progress(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"""
        if self.learning_stats['total_samples'] == 0:
            return "–û–±—É—á–µ–Ω–∏–µ –Ω–µ –Ω–∞—á–∞—Ç–æ"
        
        manual_pct = (self.learning_stats['manual_samples'] / self.learning_stats['total_samples']) * 100
        correction_pct = (self.learning_stats['correction_samples'] / self.learning_stats['total_samples']) * 100
        
        return {
            'total_samples': self.learning_stats['total_samples'],
            'accuracy': self.learning_stats['accuracy'],
            'updates': self.learning_stats['updates'],
            'manual_percentage': manual_pct,
            'correction_percentage': correction_pct,
            'buffer_size': len(self.image_buffer)
        }

class SmartObjectTracker:
    """–£–º–Ω—ã–π —Ç—Ä–µ–∫–µ—Ä —Å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é"""
    
    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        self.model = DynamicObjectClassifier().to(device)
        self.learner = DynamicLearner(self.model, device)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å
        model_path = 'smart_tracker_model.pth'
        if os.path.exists(model_path):
            try:
                self.model.load_state_dict(torch.load(model_path, map_location=device))
                print("Loaded pretrained model")
            except:
                print("Could not load pretrained model, using random weights")
        
        self.model.eval()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((64, 64)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤
        self.class_names = ['shadow/noise', 'person', 'vehicle', 'unknown']
        self.class_colors = [
            (128, 128, 128),  # —Å–µ—Ä—ã–π –¥–ª—è —Ç–µ–Ω–µ–π/—à—É–º–∞
            (255, 0, 0),      # –∫—Ä–∞—Å–Ω—ã–π –¥–ª—è –ª—é–¥–µ–π
            (0, 255, 0),      # –∑–µ–ª–µ–Ω—ã–π –¥–ª—è –º–∞—à–∏–Ω
            (0, 0, 255)       # —Å–∏–Ω–∏–π –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö
        ]
    
    def classify_object(self, frame, bbox, movement_speed=0):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤"""
        try:
            x, y, w, h = bbox
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±–ª–∞—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            x1, y1 = max(0, x), max(0, y)
            x2, y2 = min(frame.shape[1], x + w), min(frame.shape[0], y + h)
            
            if x2 <= x1 or y2 <= y1:
                return 0, 0.0  # —à—É–º
                
            image_patch = frame[y1:y2, x1:x2]
            
            if image_patch.size == 0:
                return 0, 0.0
            
            print(f"\nüéØ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–∞: {w}x{h}px, —Å–∫–æ—Ä–æ—Å—Ç—å: {movement_speed:.1f}")
            
            # 1. –ë–´–°–¢–†–ê–Ø –ü–†–û–í–ï–†–ö–ê –ù–ê –Ø–í–ù–´–ï –¢–ï–ù–ò
            if ShadowDetector.is_likely_shadow(image_patch, bbox):
                print("‚úÖ –ë—ã—Å—Ç—Ä–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: –¢–ï–ù–¨ (–¥–µ—Ç–µ–∫—Ç–æ—Ä)")
                self.learner.add_sample(image_patch, bbox, frame.shape, 0, 0.95, true_label=0, sample_type='auto')
                return 0, 0.95
            
            # 2. –ö–û–ù–¢–ï–ö–°–¢–ù–´–ô –ê–ù–ê–õ–ò–ó
            if ShadowDetector.analyze_shadow_context(frame, bbox):
                print("‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: –¢–ï–ù–¨")
                self.learner.add_sample(image_patch, bbox, frame.shape, 0, 0.85, true_label=0, sample_type='auto')
                return 0, 0.85
            
            # 3. –û–°–ù–û–í–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ü–û –ü–†–ê–í–ò–õ–ê–ú
            rule_class, rule_confidence = AdvancedObjectClassifier.classify_by_rules(
                bbox, frame.shape, movement_speed
            )
            
            # 4. –ù–ï–ô–†–û–ù–ù–ê–Ø –°–ï–¢–¨ –î–õ–Ø –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ô –ü–†–û–í–ï–†–ö–ò
            neural_class, neural_confidence = 3, 0.1
            
            try:
                image_tensor = self.transform(image_patch).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    outputs = self.model(image_tensor, bbox, frame.shape)
                    probabilities = F.softmax(outputs, dim=1)
                    confidence, predicted = torch.max(probabilities, 1)
                    
                    neural_class = predicted.item()
                    neural_confidence = confidence.item()
                    
                class_names = ['SHADOW', 'PERSON', 'VEHICLE', 'UNKNOWN']
                print(f"ü§ñ –ù–µ–π—Ä–æ–Ω–∫–∞: {class_names[neural_class]} ({neural_confidence:.2f})")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–µ–π—Ä–æ–Ω–∫–∏: {e}")
                neural_class, neural_confidence = 3, 0.1
            
            # 5. –£–ú–ù–ê–Ø –ö–û–ú–ë–ò–ù–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
            
            # –ï—Å–ª–∏ –ø—Ä–∞–≤–∏–ª–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ –∫–ª–∞—Å—Å - –¥–æ–≤–µ—Ä—è–µ–º –∏–º
            if rule_confidence > 0.8:
                final_class = rule_class
                final_confidence = rule_confidence
                print(f"üéØ –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï: –î–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª–∞–º")
                
            # –ï—Å–ª–∏ –ø—Ä–∞–≤–∏–ª–∞ –∏ –Ω–µ–π—Ä–æ–Ω–∫–∞ —Å–æ–≥–ª–∞—Å–Ω—ã - –æ–±—ä–µ–¥–∏–Ω—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            elif rule_class == neural_class and rule_confidence > 0.6:
                final_class = rule_class
                final_confidence = min(0.95, (rule_confidence + neural_confidence) / 2 + 0.1)
                print(f"üéØ –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï: –°–æ–≥–ª–∞—Å–∏–µ –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏")
                
            # –ï—Å–ª–∏ –Ω–µ–π—Ä–æ–Ω–∫–∞ –æ—á–µ–Ω—å —É–≤–µ—Ä–µ–Ω–∞, –∞ –ø—Ä–∞–≤–∏–ª–∞ —Å–æ–º–Ω–µ–≤–∞—é—Ç—Å—è
            elif neural_confidence > 0.85 and rule_confidence < 0.7:
                final_class = neural_class
                final_confidence = neural_confidence * 0.9
                print(f"üéØ –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï: –î–æ–≤–µ—Ä—è–µ–º –Ω–µ–π—Ä–æ–Ω–∫–µ (–≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)")
                
            # –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º
            elif rule_class != neural_class:
                print(f"‚ö° –ö–û–ù–§–õ–ò–ö–¢: –ü—Ä–∞–≤–∏–ª–∞={class_names[rule_class]}({rule_confidence:.2f}) vs –ù–µ–π—Ä–æ–Ω–∫–∞={class_names[neural_class]}({neural_confidence:.2f})")
                
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –¢–µ–Ω–∏ (–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –ø—Ä–µ–≤—ã—à–µ –≤—Å–µ–≥–æ)
                if rule_class == 0 and rule_confidence > 0.6:
                    final_class = 0
                    final_confidence = rule_confidence
                    print("  ‚Üí –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–µ–Ω—è–º (–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å)")
                elif neural_class == 0 and neural_confidence > 0.7:
                    final_class = 0
                    final_confidence = neural_confidence * 0.9
                    print("  ‚Üí –ù–µ–π—Ä–æ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–∞ –≤ —Ç–µ–Ω–∏")
                
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –ú–∞—à–∏–Ω—ã vs –õ—é–¥–∏ (–±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–æ –∫ –º–∞—à–∏–Ω–∞–º)
                elif rule_class == 2 and rule_confidence > 0.7:  # –ø—Ä–∞–≤–∏–ª–∞ –≥–æ–≤–æ—Ä—è—Ç –º–∞—à–∏–Ω–∞
                    final_class = 2
                    final_confidence = rule_confidence
                    print("  ‚Üí –ü—Ä–∞–≤–∏–ª–∞ —É–≤–µ—Ä–µ–Ω—ã –≤ –º–∞—à–∏–Ω–µ")
                elif neural_class == 2 and neural_confidence > 0.8:  # –Ω–µ–π—Ä–æ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–∞ –≤ –º–∞—à–∏–Ω–µ
                    final_class = 2
                    final_confidence = neural_confidence * 0.85
                    print("  ‚Üí –ù–µ–π—Ä–æ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–∞ –≤ –º–∞—à–∏–Ω–µ")
                
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –õ—é–¥–∏ (–±–æ–ª–µ–µ –º—è–≥–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏)
                elif rule_class == 1 and rule_confidence > 0.6:
                    final_class = 1
                    final_confidence = rule_confidence
                    print("  ‚Üí –ü—Ä–∞–≤–∏–ª–∞ –≥–æ–≤–æ—Ä—è—Ç —á–µ–ª–æ–≤–µ–∫")
                elif neural_class == 1 and neural_confidence > 0.7:
                    final_class = 1
                    final_confidence = neural_confidence * 0.85
                    print("  ‚Üí –ù–µ–π—Ä–æ–Ω–∫–∞ –≥–æ–≤–æ—Ä–∏—Ç —á–µ–ª–æ–≤–µ–∫")
                
                # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –≤—ã–±–∏—Ä–∞–µ–º –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥
                else:
                    if rule_confidence > neural_confidence:
                        final_class = rule_class
                        final_confidence = rule_confidence * 0.8
                        print("  ‚Üí –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞ (–≤—ã—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)")
                    else:
                        final_class = neural_class
                        final_confidence = neural_confidence * 0.8
                        print("  ‚Üí –í—ã–±–∏—Ä–∞–µ–º –Ω–µ–π—Ä–æ–Ω–∫—É (–≤—ã—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)")
            
            # –ï—Å–ª–∏ –ø—Ä–∞–≤–∏–ª–∞ –¥–∞–ª–∏ –Ω–∏–∑–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            else:
                final_class = rule_class
                final_confidence = rule_confidence
                print(f"üéØ –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï: –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∞–≤–∏–ª–∞")
            
            # 6. –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ù–ê –ê–†–¢–ï–§–ê–ö–¢–´
            area = w * h
            aspect_ratio = w / h if h > 0 else 1
            
            if (area < 400 or aspect_ratio > 8 or aspect_ratio < 0.12) and final_class != 0:
                print("üö´ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π/—Å—Ç—Ä–∞–Ω–Ω—ã–π")
                final_class = 0
                final_confidence = 0.8
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
            final_class_name = ['–¢–ï–ù–¨', '–ß–ï–õ–û–í–ï–ö', '–ú–ê–®–ò–ù–ê', '–ù–ï–ò–ó–í–ï–°–¢–ù–û'][final_class]
            print(f"üèÅ –ò–¢–û–ì: {final_class_name} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {final_confidence:.2f})")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            if final_confidence > 0.75 or final_class == 0:
                self.learner.add_sample(image_patch, bbox, frame.shape, 
                                      final_class, final_confidence, sample_type='auto')
            
            return final_class, final_confidence
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            return 0, 0.7
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å"""
        torch.save(self.model.state_dict(), 'smart_tracker_model.pth')
        print("Model saved")
    
    def get_stats(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—É—á–µ–Ω–∏—è"""
        return self.learner.learning_stats