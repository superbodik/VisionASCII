import threading
import queue
import time
import os

import cv2
import numpy as np
import tkinter as tk
from gui import VisionGUI
from camera import open_camera, get_frame, close_camera, apply_zoom
from neural_classifier import  SmartTrackerWithTraining
from interactive_training import SmartObjectTracker

frame_queue = queue.Queue(maxsize=10)
stop_flag = threading.Event()

last_time = time.time()
frame_count = 0

# –£–º–Ω—ã–π —Ç—Ä–µ–∫–µ—Ä —Å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º
smart_tracker = SmartObjectTracker()
trainer_system = SmartTrackerWithTraining(smart_tracker)

# –°–ø–∏—Å–æ–∫ —Ç—Ä–µ–∫–µ—Ä–æ–≤
trackers = []
max_trackers = 8
next_tracker_id = 1

floor = 3
max_zoom = 5.0
prev_gray = None

def detect_motion(prev_gray, current_gray, threshold=5000):
    diff = cv2.absdiff(prev_gray, current_gray)
    changed_pixels = np.count_nonzero(diff > 25)
    return changed_pixels > threshold

def create_tracker():
    """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π —Ç—Ä–µ–∫–µ—Ä"""
    try:
        return cv2.legacy.TrackerCSRT_create()
    except AttributeError:
        return cv2.TrackerCSRT_create()

def calculate_distance(box1, box2):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–∞–º–∏ –¥–≤—É—Ö –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–æ–≤"""
    x1, y1, w1, h1 = box1
    x2, y2, w2, h2 = box2
    
    center1 = (x1 + w1//2, y1 + h1//2)
    center2 = (x2 + w2//2, y2 + h2//2)
    
    return np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)

def boxes_overlap_iou(box1, box2, threshold=0.25):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –ø–æ IoU"""
    x1, y1, w1, h1 = box1
    x2, y2, w2, h2 = box2
    
    x_left = max(x1, x2)
    y_top = max(y1, y2)
    x_right = min(x1 + w1, x2 + w2)
    y_bottom = min(y1 + h1, y2 + h2)
    
    if x_right < x_left or y_bottom < y_top:
        return False
    
    intersection = (x_right - x_left) * (y_bottom - y_top)
    area1 = w1 * h1
    area2 = w2 * h2
    union = area1 + area2 - intersection
    
    iou = intersection / union if union > 0 else 0
    return iou > threshold

def is_too_close_to_existing(new_box, existing_boxes, min_distance=80):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –±–ª–∏–∑–æ—Å—Ç—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –æ–±—ä–µ–∫—Ç–∞–º"""
    for existing_box in existing_boxes:
        if calculate_distance(new_box, existing_box) < min_distance:
            return True
        if boxes_overlap_iou(new_box, existing_box, threshold=0.2):
            return True
    return False

def get_display_info(class_id, confidence):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–ª–∞—Å—Å–∞"""
    class_names = ['SHADOW', 'PERSON', 'VEHICLE', 'UNKNOWN']
    class_colors = [
        (128, 128, 128),  # —Å–µ—Ä—ã–π –¥–ª—è —Ç–µ–Ω–µ–π
        (0, 0, 255),      # –∫—Ä–∞—Å–Ω—ã–π –¥–ª—è –ª—é–¥–µ–π  
        (0, 255, 0),      # –∑–µ–ª–µ–Ω—ã–π –¥–ª—è –º–∞—à–∏–Ω
        (255, 0, 255)     # –ø—É—Ä–ø—É—Ä–Ω—ã–π –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö
    ]
    
    name = class_names[class_id] if 0 <= class_id < len(class_names) else 'UNKNOWN'
    color = class_colors[class_id] if 0 <= class_id < len(class_colors) else (255, 255, 255)
    
    return name, color

def capture_thread_func(gui):
    global last_time, frame_count, trackers, prev_gray, next_tracker_id, smart_tracker

    cap = open_camera()
    os.makedirs("snapshots", exist_ok=True)
    os.makedirs("training_data", exist_ok=True)

    # –°—á–µ—Ç—á–∏–∫ –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    save_counter = 0

    while not stop_flag.is_set():
        frame = get_frame(cap)
        if frame is None:
            time.sleep(0.01)
            continue

        zoom_factor = gui.cached_zoom
        motion_threshold = gui.cached_motion_threshold
        save_enabled = gui.cached_save_enabled

        frame = apply_zoom(frame, zoom_factor)

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray, (3, 3), 0)

        if prev_gray is None:
            prev_gray = blurred
            continue

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç—Ä–µ–∫–µ—Ä—ã
        active_trackers = []
        active_boxes = []
        
        for tracker_data in trackers:
            tracker = tracker_data['tracker']
            tracker_id = tracker_data['id']
            neural_class = tracker_data['neural_class']
            confidence = tracker_data['confidence']
            position_history = tracker_data['history']
            fail_count = tracker_data.get('fail_count', 0)
            
            success, box = tracker.update(frame)
            if success:
                x, y, w, h = [int(v) for v in box]
                current_box = (x, y, w, h)
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç (–∫–∞–∂–¥—ã–µ 30 –∫–∞–¥—Ä–æ–≤)
                if frame_count % 30 == 0:
                    # –í—ã—á–∏—Å–ª—è–µ–º —Ç–µ–∫—É—â—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                    current_speed = 0
                    if len(position_history) >= 2:
                        dx = position_history[-1][0] - position_history[-2][0]
                        dy = position_history[-1][1] - position_history[-2][1]
                        current_speed = np.sqrt(dx*dx + dy*dy)
                    
                    new_class, new_confidence = trainer_system.classify_object(frame, current_box, current_speed)
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–ª–∞—Å—Å —Å –±–æ–ª–µ–µ –º—è–≥–∫–∏–º –ø–æ—Ä–æ–≥–æ–º
                    if new_confidence > 0.4:
                        neural_class = new_class
                        confidence = new_confidence
                        tracker_data['neural_class'] = neural_class
                        tracker_data['confidence'] = confidence
                
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ç–µ–Ω–∏ –∏ —à—É–º (–∫–ª–∞—Å—Å 0)
                if neural_class == 0:  # shadow/noise
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –æ–±—ä–µ–∫—Ç–∞
                if w * h >= 200 and w >= 10 and h >= 10:
                    center = (x + w // 2, y + h // 2)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ–∑–∏—Ü–∏–π
                    position_history.append(center)
                    if len(position_history) > 10:
                        position_history.pop(0)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–≤–∏–∂–µ–Ω–∏–µ (—Ä–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤)
                    movement_threshold = {
                        1: 0.5,  # person - –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥
                        2: 1.2,  # vehicle - —Å—Ä–µ–¥–Ω–∏–π –ø–æ—Ä–æ–≥
                        3: 0.8   # unknown - –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥
                    }.get(neural_class, 0.8)
                    
                    is_moving = False
                    if len(position_history) >= 3:
                        recent_movements = []
                        for i in range(max(1, len(position_history)-3), len(position_history)):
                            dx = position_history[i][0] - position_history[i-1][0]
                            dy = position_history[i][1] - position_history[i-1][1]
                            movement = np.sqrt(dx*dx + dy*dy)
                            recent_movements.append(movement)
                        
                        avg_movement = np.mean(recent_movements)
                        is_moving = avg_movement > movement_threshold
                    
                    if is_moving:
                        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å
                        if len(position_history) >= 2:
                            dx = position_history[-1][0] - position_history[-2][0]
                            dy = position_history[-1][1] - position_history[-2][1]
                            speed = np.sqrt(dx*dx + dy*dy)
                        else:
                            speed = 0
                        
                        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        class_name, color = get_display_info(neural_class, confidence)
                        
                        # –¢–æ–ª—â–∏–Ω–∞ —Ä–∞–º–∫–∏ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                        thickness = max(1, int(confidence * 4))
                        
                        # –†–∏—Å—É–µ–º —Ç—Ä–µ–∫–µ—Ä
                        cv2.rectangle(frame, (x, y), (x + w, y + h), color, thickness)
                        
                        # –ü–æ–¥–ø–∏—Å—å —Å –∫–ª–∞—Å—Å–æ–º, ID, —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
                        label = f"{class_name}-{tracker_id} S:{speed:.1f} C:{confidence:.2f}"
                        cv2.putText(frame, label, (x, y - 10), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–∫—Ç–∏–≤–Ω—ã–π —Ç—Ä–µ–∫–µ—Ä
                        tracker_data['history'] = position_history
                        tracker_data['fail_count'] = 0
                        active_trackers.append(tracker_data)
                        active_boxes.append(current_box)
                    else:
                        # –û–±—ä–µ–∫—Ç –Ω–µ –¥–≤–∏–∂–µ—Ç—Å—è
                        fail_count += 1
                        max_static_frames = 25 if neural_class == 2 else 20  # –±–æ–ª—å—à–µ —à–∞–Ω—Å–æ–≤ –¥–ª—è –º–∞—à–∏–Ω
                        if fail_count < max_static_frames:
                            tracker_data['fail_count'] = fail_count
                            active_trackers.append(tracker_data)
                else:
                    # –ù–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ä–∞–∑–º–µ—Ä
                    pass
            else:
                # –¢—Ä–µ–∫–µ—Ä –ø–æ—Ç–µ—Ä—è–ª –æ–±—ä–µ–∫—Ç
                fail_count += 1
                if fail_count < 10:
                    tracker_data['fail_count'] = fail_count
                    active_trackers.append(tracker_data)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —Ç—Ä–µ–∫–µ—Ä–æ–≤
        trackers = active_trackers

        # –ò—â–µ–º –Ω–æ–≤—ã–µ –æ–±—ä–µ–∫—Ç—ã
        if len(trackers) < max_trackers and detect_motion(prev_gray, blurred, threshold=motion_threshold):
            diff = cv2.absdiff(prev_gray, blurred)
            _, thresh = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)
            
            # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
            kernel = np.ones((3,3), np.uint8)
            thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)
            thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)
            
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            contours = sorted(contours, key=cv2.contourArea, reverse=True)
            
            new_trackers_added = 0
            for c in contours:
                if new_trackers_added >= 2:
                    break
                    
                area = cv2.contourArea(c)
                if area > 250:  # –µ—â–µ –±–æ–ª—å—à–µ —Å–Ω–∏–∂–µ–Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—â–∞–¥—å
                    x, y, w, h = cv2.boundingRect(c)
                    new_box = (x, y, w, h)
                    
                    # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
                    neural_class, confidence = trainer_system.classify_object(frame, new_box, movement_speed=0)
                    
                    # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏–Ω—è—Ç–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
                    if neural_class != 0 and confidence > 0.25:  # –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–µ–Ω –ø–æ—Ä–æ–≥
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–ª–∏–∑–æ—Å—Ç—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º
                        min_distance = 100 if neural_class == 2 else 60  # —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –º–∞—à–∏–Ω –∏ –ª—é–¥–µ–π
                        
                        if not is_too_close_to_existing(new_box, active_boxes, min_distance):
                            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç—Ä–µ–∫–µ—Ä
                            new_tracker = create_tracker()
                            if new_tracker.init(frame, (x, y, w, h)):
                                center = (x + w // 2, y + h // 2)
                                tracker_data = {
                                    'tracker': new_tracker,
                                    'history': [center],
                                    'id': next_tracker_id,
                                    'neural_class': neural_class,
                                    'confidence': confidence,
                                    'fail_count': 0
                                }
                                trackers.append(tracker_data)
                                next_tracker_id += 1
                                new_trackers_added += 1
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–∑–µ—Ü –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
                                if save_enabled:
                                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                                    microseconds = str(int(time.time() * 1000000) % 1000000)
                                    full_timestamp = f"{timestamp}_{microseconds}"
                                    class_name = ['shadow', 'person', 'vehicle', 'unknown'][neural_class]
                                    
                                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å—å –∫–∞–¥—Ä –≤ snapshots
                                    cv2.imwrite(f"snapshots/{class_name}_{full_timestamp}.jpg", frame)
                                    
                                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—ã—Ä–µ–∑–∞–Ω–Ω—É—é –æ–±–ª–∞—Å—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ training_data
                                    x1, y1 = max(0, x), max(0, y)
                                    x2, y2 = min(frame.shape[1], x + w), min(frame.shape[0], y + h)
                                    crop = frame[y1:y2, x1:x2]
                                    if crop.size > 0:
                                        cv2.imwrite(f"training_data/{class_name}_{full_timestamp}_crop.jpg", crop)

        prev_gray = blurred

        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        class_counts = [0, 0, 0, 0]  # shadow, person, vehicle, unknown
        for t in trackers:
            class_id = t['neural_class']
            if 0 <= class_id < len(class_counts):
                class_counts[class_id] += 1
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—É—á–µ–Ω–∏—è
        learning_stats = trainer_system.smart_tracker.get_stats()
        
        # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ç–µ–∫—É—â–∏—Ö —Ç—Ä–µ–∫–µ—Ä–∞—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            current_detections = []
            for tracker_data in trackers:
                if 'neural_class' in tracker_data and 'confidence' in tracker_data:
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π bbox –∏–∑ —Ç—Ä–µ–∫–µ—Ä–∞
                    tracker = tracker_data['tracker']
                    success, box = tracker.update(frame.copy())
                    if success:
                        x, y, w, h = [int(v) for v in box]
                        detection_data = {
                            'bbox': (x, y, w, h),
                            'neural_class': tracker_data['neural_class'],
                            'confidence': tracker_data['confidence']
                        }
                        current_detections.append(detection_data)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–¥—Ä –≤ —Å–∏—Å—Ç–µ–º–µ –æ–±—É—á–µ–Ω–∏—è
            trainer_system.process_frame(frame, current_detections)
        except Exception as e:
            print(f"Training system error: {e}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ —ç–∫—Ä–∞–Ω–µ
        info_lines = [
            f"People: {class_counts[1]} | Vehicles: {class_counts[2]} | Unknown: {class_counts[3]}",
            f"Shadows filtered: {class_counts[0]} | Learning updates: {learning_stats['updates']}",
            f"Model accuracy: {learning_stats['accuracy']:.3f} | Confidence threshold: 0.5+"
        ]
        
        for i, line in enumerate(info_lines):
            y_pos = 25 + i * 25
            cv2.putText(frame, line, (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            cv2.putText(frame, line, (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)

        # –£–º–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏
        save_counter += 1
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—â–µ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∞–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è
        try:
            progress = trainer_system.smart_tracker.learner.get_learning_progress()
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            if progress['manual_samples'] + progress['correction_samples'] > 0:
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä—É—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—â–µ
                save_interval = 200
            else:
                # –û–±—ã—á–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞
                save_interval = 500
                
            if save_counter % save_interval == 0:
                trainer_system.smart_tracker.save_model()
                print(f"üîÑ Model auto-saved: {progress['total_samples']} samples, "
                      f"{progress['accuracy']:.1%} accuracy")
                      
        except Exception as e:
            # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—é
            if save_counter % 500 == 0:
                try:
                    trainer_system.smart_tracker.save_model()
                except Exception as save_error:
                    print(f"Error saving model: {save_error}")

        try:
            frame_queue.put_nowait(frame)
        except queue.Full:
            pass

        frame_count += 1
        now_time = time.time()
        if now_time - last_time >= 1.0:
            fps = frame_count / (now_time - last_time)
            frame_count = 0
            last_time = now_time
            if gui:
                gui.root.after(0, gui.update_fps, fps)

        time.sleep(0.008)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
    try:
        trainer_system.smart_tracker.save_model()
        print("Model saved on exit")
    except Exception as e:
        print(f"Error saving model on exit: {e}")

    close_camera(cap)

def main():
    print("üöÄ Starting Smart AI Tracker with Dynamic Learning...")
    print("üìö Classes: SHADOW (filtered) | PERSON (red) | VEHICLE (green) | UNKNOWN (purple)")
    print("üß† The system learns from your corrections and improves in real-time!")
    print("üéØ Interactive training window appears every 10 seconds")
    print("üí° Click objects to correct them, or draw new areas to train on")
    
    root = tk.Tk()
    root.title("Smart AI Tracker - Dynamic Learning System")
    
    gui = VisionGUI(root, frame_queue)

    t = threading.Thread(target=capture_thread_func, args=(gui,), daemon=True)
    t.start()

    try:
        root.mainloop()
    except KeyboardInterrupt:
        print("üõë Shutting down...")
    finally:
        stop_flag.set()
        t.join()
        print("‚úÖ System shutdown complete")

if __name__ == "__main__":
    main()